# ============================================
# OpenMemory - Environment Configuration
# ============================================

# --------------------------------------------
# Backend Server Settings
# --------------------------------------------
OM_PORT=8080

# API Authentication (IMPORTANT: Set a strong API key for production!)
# Generate a secure key: openssl rand -base64 32
# Leave empty to disable authentication (development only)
OM_API_KEY=your-secret-api-key-here

# Rate Limiting
# Enable rate limiting to prevent abuse
OM_RATE_LIMIT_ENABLED=true
# Time window in milliseconds (default: 60000 = 1 minute)
OM_RATE_LIMIT_WINDOW_MS=60000
# Maximum requests per window (default: 100 requests per minute)
OM_RATE_LIMIT_MAX_REQUESTS=100

# Optional: Log all authenticated requests (set to 'true' for debugging)
OM_LOG_AUTH=false

# Server Mode
OM_MODE=standard # development | production | standard | langgraph

# --------------------------------------------
# Metadata Store
# --------------------------------------------
# sqlite (default) | postgres
OM_METADATA_BACKEND=sqlite
OM_DB_PATH=./data/openmemory.sqlite

# PostgreSQL Settings (used when OM_METADATA_BACKEND=postgres or OM_VECTOR_BACKEND=pgvector)
OM_PG_HOST=localhost
OM_PG_PORT=5432
OM_PG_DB=openmemory
OM_PG_USER=postgres
OM_PG_PASSWORD=postgres
OM_PG_SCHEMA=public
OM_PG_TABLE=openmemory_memories
OM_PG_SSL=disable # disable | require

# Alternative: use a libpq URI (preferred for cross-platform deployments)
# Only simple host:port/database URIs supported; socket-style (host=/path) and multi-host libpq features are ignored in favor of discrete OM_PG_* variables
# Example: postgresql://user:password@host:5432/openmemory?sslmode=require
# OM_PG_CONNECTION_STRING=postgresql://postgres:pass@127.0.0.1:5432/openmemory?sslmode=require

# --------------------------------------------
# Vector Store Backend
# --------------------------------------------
# sqlite (default) | pgvector | weaviate
OM_VECTOR_BACKEND=sqlite
OM_VECTOR_TABLE=openmemory_vectors
OM_WEAVIATE_URL=
OM_WEAVIATE_API_KEY=
OM_WEAVIATE_CLASS=OpenMemory

# --------------------------------------------
# Embeddings Configuration
# --------------------------------------------
# Available providers: openai, gemini, ollama, local, synthetic
# Embedding models per sector can be configured in models.yaml
#
# NOTE: Your selected TIER (fast/smart/deep) affects how embeddings work:
# • FAST tier: Uses synthetic embeddings regardless of OM_EMBEDDINGS setting
# • SMART tier: Combines synthetic + compressed semantic from your chosen provider
# • DEEP tier: Uses full embeddings from your chosen provider
#
# For SMART/DEEP tiers, set your preferred provider:
OM_EMBEDDINGS=openai

# Vector dimension (auto-adjusted by tier, but can be overridden)
# • FAST: 256-dim  • SMART: 384-dim  • DEEP: 1536-dim
# OM_VEC_DIM=1536

# Embedding Mode
# simple   = 1 unified batch call for all sectors (faster, rate-limit safe, recommended)
# advanced = 5 separate calls, one per sector (higher precision, more API calls)
OM_EMBED_MODE=simple

# Advanced Mode Options (only used when OM_EMBED_MODE=advanced)
# Enable parallel embedding (not recommended for Gemini due to rate limits)
OM_ADV_EMBED_PARALLEL=false
# Delay between embeddings in milliseconds
OM_EMBED_DELAY_MS=200

# --------------------------------------------
# Router-Based CPU Embedding (Advanced)
# --------------------------------------------
# Lightweight CPU-only router that selects optimal models per sector
# Features: Sector-based model routing, SIMD acceleration (20-30% faster), graceful fallback
# Performance: P95 <150ms CPU, automatic Ollama management, hybrid fusion enabled
#
# Set OM_EMBED_KIND=router_cpu to enable router-based embeddings (requires Ollama sidecar):
# OM_EMBED_KIND=router_cpu
#
# Router-specific configuration (only effective when OM_EMBED_KIND=router_cpu):
# Cache TTL for router model selections (30 seconds recommended)
OM_ROUTER_CACHE_TTL_MS=30000
# Enable synthetic fallback when Ollama models are unavailable (recommended: true)
OM_ROUTER_FALLBACK_ENABLED=true
# Enable SIMD acceleration specifically for router CPU embedding operations (20-30% faster vector fusion)
# Falls back to OM_SIMD_ENABLED if not set; use this for fine-grained router control
OM_ROUTER_SIMD_ENABLED=true
# Enable SIMD-accelerated vector operations for non-router paths (20-30% performance boost on supported CPUs)
OM_SIMD_ENABLED=true

# Custom sector-to-model mappings (leave empty for defaults, JSON format)
# Default mappings: episodic→nomic-embed-text, semantic→nomic-embed-text, procedural→bge-small-en-v1.5, emotional→nomic-embed-text, reflective→nomic-embed-text
# OM_ROUTER_SECTOR_MODELS={"episodic":"nomic-embed-text","semantic":"nomic-embed-text","procedural":"bge-small-en-v1.5","emotional":"nomic-embed-text","reflective":"nomic-embed-text"}
# Enable cold startup validation (checks model dimensions, may slow startup)
OM_ROUTER_VALIDATE_ON_START=false
# Dimension tolerance for router models (0.1 = 10% tolerance before fallback)
OM_ROUTER_DIM_TOLERANCE=0.1

# OpenAI-compatible Embeddings Provider
# OM_OPENAI_BASE_URL=https://api.openai.com/v1
# Model override for all sector embeddings (leave empty to use defaults)
# OM_OPENAI_MODEL=text-embedding-qwen3-embedding-4b

# API Configuration
# Max request body size in bytes (default: 1MB)
OM_MAX_PAYLOAD_SIZE=1000000

# --------------------------------------------
# Embedding Provider API Keys
# --------------------------------------------
# OpenAI Embeddings
OPENAI_API_KEY=your-openai-api-key-here

# Google Gemini Embeddings
GEMINI_API_KEY=your-gemini-api-key-here

# Ollama Local Embeddings
# When using docker-compose, the sidecar service is available at http://ollama:11434
# For local development without Docker, use http://localhost:11434
OLLAMA_URL=http://localhost:11434

# Ollama SideCar Configuration (docker-compose only)
# Default models to keep in memory (comma-separated, e.g., "nomic-embed-text,llava:13b")
OM_OLLAMA_MODELS=nomic-embed-text
# How long to keep models loaded in memory (5m, 10m, -1 for always)
OM_OLLAMA_KEEP_ALIVE=5m
# Number of parallel requests Ollama can handle (default: 1)
OM_OLLAMA_NUM_PARALLEL=1
# Number of GPUs to use for inference (0 = CPU only, -1 = all available)
OM_OLLAMA_NUM_GPU=0
# Enable automatic model pulling on startup (when using agentic workflows)
OM_OLLAMA_AUTO_PULL=true
# Enable multimodal models support (LLaVA, Whisper, etc.)
OM_OLLAMA_MULTIMODAL_ENABLED=false

# Local Model Path (for custom embedding models)
LOCAL_MODEL_PATH=/path/to/your/local/model

# --------------------------------------------
# Universal Auth / JWT / OIDC (Optional)
# --------------------------------------------
# Set OM_AUTH_PROVIDER to 'jwt' or 'supabase' to enable authentication; unset results in API-key-only authentication
# For OM_AUTH_PROVIDER=jwt, a missing OM_JWT_SECRET will cause startup failure in production modes
# OM_AUTH_PROVIDER=jwt  # jwt | supabase
# OM_JWT_SECRET=your-jwt-secret-or-jwks-uri
# OM_JWT_ISSUER=https://auth.example.com
# OM_JWT_AUDIENCE=urn:openmemory

# --------------------------------------------
# S3-Compatible Bucket Configuration
# --------------------------------------------
# For bucket providers, missing credentials will result in warnings and disabled bucket functionality rather than a hard error
# OM_BUCKET_PROVIDER=minio # minio | supabase | s3
# OM_BUCKET_ENDPOINT=http://minio:9000
# OM_BUCKET_ACCESS_KEY=minioadmin
# OM_BUCKET_SECRET_KEY=minioadmin
# OM_BUCKET_REGION=us-east-1
# OM_BUCKET_FORCE_PATH_STYLE=true
# OM_BUCKET_NAME=openmemory-files

# --------------------------------------------
# SQLite Backup Configuration
# --------------------------------------------
# Local backup storage directory (default: ./data/backups)
OM_BACKUP_DIR=./data/backups
# Automatic cleanup of old backups (days to retain, default: 7)
OM_BACKUP_RETENTION_DAYS=7
# Enable backup upload to Supabase Storage (default: false)
OM_BACKUP_CLOUD_ENABLED=false
# Enable automatic scheduled backups (default: false)
OM_BACKUP_AUTO_SCHEDULE=false
# Cron expression for backup scheduling (default: daily at 2 AM)
OM_BACKUP_SCHEDULE_CRON=0 2 * * *

# --------------------------------------------
# GPU Optimization examples for Ollama
# --------------------------------------------
# OM_OLLAMA_NUM_GPU=1
# OLLAMA_FLASH_ATTENTION=1
# OLLAMA_KV_CACHE_TYPE=q8_0
# OLLAMA_VULKAN=1  # AMD Vulkan backend

# --------------------------------------------
# Memory System Settings
# --------------------------------------------

# ============================================
# PERFORMANCE TIER (Manual Configuration Required)
# ============================================
# OpenMemory requires you to manually set the performance tier.
# Set OM_TIER to one of: hybrid, fast, smart, or deep
#
# Available Tiers:
#
# HYBRID - Keyword + Synthetic embeddings (256-dim) with BM25 ranking
#         • Recall: ~100% (exact keyword matching)  • QPS: 800-1000  • RAM: 0.5GB/10k memories
#         • Best for: Exact searches, documentation, code search, personal knowledge
#         • Features: Exact phrase matching, BM25 scoring, n-gram matching, 100% accuracy
#         • Use when: You need guaranteed exact matches and keyword-based retrieval
#
# FAST  - Synthetic embeddings only (256-dim)
#         • Recall: ~70-75%  • QPS: 700-850  • RAM: 0.6GB/10k memories
#         • Best for: Local apps, VS Code extensions, low-end hardware
#         • Use when: < 4 CPU cores or < 8GB RAM
#
# SMART - Hybrid embeddings (256-dim synthetic + 128-dim compressed semantic = 384-dim)
#         • Recall: ~85%  • QPS: 500-600  • RAM: 0.9GB/10k memories
#         • Best for: Production servers, AI copilots, mid-range hardware
#         • Use when: 4-7 CPU cores and 8-15GB RAM
#
# DEEP  - Full AI embeddings (1536-dim OpenAI/Gemini)
#         • Recall: ~95-100%  • QPS: 350-400  • RAM: 1.6GB/10k memories
#         • Best for: Cloud deployments, high-accuracy systems, semantic research
#         • Use when: 8+ CPU cores and 16+ GB RAM
#
# REQUIRED: Set your tier (no auto-detection):
OM_TIER=hybrid

# Keyword Matching Settings (HYBRID tier only)
# Boost multiplier for keyword matches (default: 2.5)
OM_KEYWORD_BOOST=2.5
# Minimum keyword length for matching (default: 3)
OM_KEYWORD_MIN_LENGTH=3

OM_MIN_SCORE=0.3

# ============================================
# Smart Decay Settings (Time-Based Algorithm)
# ============================================
# Decay interval in minutes - how often the decay cycle runs
# The new algorithm uses time-based decay with daily lambda rates (hot=0.005/day, warm=0.02/day, cold=0.05/day)
# Unlike batch-based systems, running more frequently doesn't increase decay speed
# Decay is calculated from: decay_factor = exp(-lambda * days_since_access / (salience + 0.1))
#
# Recommended intervals:
# • Testing: 30 minutes (for rapid validation)
# • Development: 60-120 minutes (balanced testing)
# • Production: 120-180 minutes (optimal - captures meaningful decay deltas while minimizing overhead)
#
# At 2-3 hours: hot tier decays ~0.04-0.06%, warm ~0.16-0.24%, cold ~0.4-0.6% per cycle
OM_DECAY_INTERVAL_MINUTES=120

# Number of parallel decay worker threads (default: 3)
OM_DECAY_THREADS=3
# Cold tier threshold - memories below this salience get fingerprinted (default: 0.25)
OM_DECAY_COLD_THRESHOLD=0.25
# Reinforce memory salience when queried (default: true)
OM_DECAY_REINFORCE_ON_QUERY=true
# Enable regeneration of cold memories on query hits (default: true)
OM_REGENERATION_ENABLED=true
# Maximum vector dimensions (default: 1536)
OM_MAX_VECTOR_DIM=1536
# Minimum vector dimensions for compression (default: 64)
OM_MIN_VECTOR_DIM=64
# Number of summary compression layers 1-3 (default: 3)
OM_SUMMARY_LAYERS=3

# Full Semantic Graph MVP Settings
# Use summary-only storage (≤300 chars, intelligent extraction)
OM_USE_SUMMARY_ONLY=true
# Maximum summary length - smart extraction preserves dates, names, numbers, actions
OM_SUMMARY_MAX_LENGTH=300
# Memories per segment (10k recommended for optimal cache performance)
OM_SEG_SIZE=10000

# Cache segments (auto-tuned by tier, but can be overridden)
# • FAST: 2 segments  • SMART: 3 segments  • DEEP: 5 segments
# OM_CACHE_SEGMENTS=3

# Max active queries (auto-tuned by tier, but can be overridden)
# • FAST: 32 queries  • SMART: 64 queries  • DEEP: 128 queries
# OM_MAX_ACTIVE=64

# Brain Sector Configuration (auto-classified, but you can override)
# Sectors: episodic, semantic, procedural, emotional, reflective

# Auto-Reflection System
# Automatically creates reflective memories by clustering similar memories
OM_AUTO_REFLECT=false
# Reflection interval in minutes (default: 10)
OM_REFLECT_INTERVAL=10
# Minimum memories required before reflection runs (default: 20)
OM_REFLECT_MIN_MEMORIES=20

# Compression
# Enable automatic content compression for large memories
OM_COMPRESSION_ENABLED=false
# Minimum content length (characters) to trigger compression (default: 100)
OM_COMPRESSION_MIN_LENGTH=100
# Compression algorithm: semantic, syntactic, aggressive, auto (default: auto)
OM_COMPRESSION_ALGORITHM=auto

# --------------------------------------------
# LangGraph Integration Mode (LGM)
# --------------------------------------------
OM_LG_NAMESPACE=default
OM_LG_MAX_CONTEXT=50
OM_LG_REFLECTIVE=true
