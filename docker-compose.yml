version: "3.8"

# Podman compatibility notes:
# - Podman supports docker-compose via podman-compose or `podman play kube` after conversion.
# - When using Podman on SELinux hosts, append `:Z` to bind mounts (see podman/README.md).
# - Some CI environments expect Docker; for Podman use the Podman-compatible invocation in podman/README.md.

services:
  openmemory:
    build:
      context: ./backend
      dockerfile: Dockerfile
    ports:
      - "8080:8080"
    environment:
      # Core Configuration
      - OM_PORT=${OM_PORT:-8080}
      - OM_MODE=${OM_MODE:-standard}
      - OM_TIER=${OM_TIER:-hybrid}
      - OM_HYBRID_FUSION=${OM_HYBRID_FUSION:-true}
      - OM_DB_PATH=${OM_DB_PATH:-/data/openmemory.sqlite}
      - OM_API_KEY=${OM_API_KEY:-}
      - OM_IDE_MODE=${OM_IDE_MODE:-false}
      - OM_IDE_ALLOWED_ORIGINS=${OM_IDE_ALLOWED_ORIGINS:-http://localhost:5173,http://localhost:3000}

      # Rate Limiting
      - OM_RATE_LIMIT_ENABLED=${OM_RATE_LIMIT_ENABLED:-false}
      - OM_RATE_LIMIT_WINDOW_MS=${OM_RATE_LIMIT_WINDOW_MS:-60000}
      - OM_RATE_LIMIT_MAX_REQUESTS=${OM_RATE_LIMIT_MAX_REQUESTS:-100}

      # Compression
      - OM_COMPRESSION_ENABLED=${OM_COMPRESSION_ENABLED:-false}
      - OM_COMPRESSION_ALGORITHM=${OM_COMPRESSION_ALGORITHM:-auto}
      - OM_COMPRESSION_MIN_LENGTH=${OM_COMPRESSION_MIN_LENGTH:-100}

      # Embeddings Configuration
      - OM_EMBEDDINGS=${OM_EMBEDDINGS:-synthetic}
      - OM_EMBED_MODE=${OM_EMBED_MODE:-simple}
      - OM_ADV_EMBED_PARALLEL=${OM_ADV_EMBED_PARALLEL:-false}
      - OM_EMBED_DELAY_MS=${OM_EMBED_DELAY_MS:-200}
      - OM_VEC_DIM=${OM_VEC_DIM:-256}

      # OpenAI Provider
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}
      - OM_OPENAI_API_KEY=${OM_OPENAI_API_KEY:-}
      - OM_OPENAI_BASE_URL=${OM_OPENAI_BASE_URL:-https://api.openai.com/v1}
      - OM_OPENAI_MODEL=${OM_OPENAI_MODEL:-}

      # Gemini Provider
      - GEMINI_API_KEY=${GEMINI_API_KEY:-}
      - OM_GEMINI_API_KEY=${OM_GEMINI_API_KEY:-}

      # Ollama Provider (SideCar deployment)
      - OLLAMA_URL=${OLLAMA_URL:-http://ollama:11434}
      - OM_OLLAMA_URL=${OM_OLLAMA_URL:-http://ollama:11434}
      - OM_OLLAMA_MODELS=${OM_OLLAMA_MODELS:-nomic-embed-text}
      - OM_OLLAMA_KEEP_ALIVE=${OM_OLLAMA_KEEP_ALIVE:-5m}
      - OM_OLLAMA_NUM_PARALLEL=${OM_OLLAMA_NUM_PARALLEL:-1}
      - OM_OLLAMA_NUM_GPU=${OM_OLLAMA_NUM_GPU:-0}

      # Local Model
      - LOCAL_MODEL_PATH=${LOCAL_MODEL_PATH:-}
      - OM_LOCAL_MODEL_PATH=${OM_LOCAL_MODEL_PATH:-}

      # Memory & Search
      - OM_MIN_SCORE=${OM_MIN_SCORE:-0.3}
      - OM_MAX_PAYLOAD_SIZE=${OM_MAX_PAYLOAD_SIZE:-1000000}

      # LangGraph Configuration
      - OM_LG_NAMESPACE=${OM_LG_NAMESPACE:-default}
      - OM_LG_MAX_CONTEXT=${OM_LG_MAX_CONTEXT:-50}
      - OM_LG_REFLECTIVE=${OM_LG_REFLECTIVE:-true}

      # Metadata Backend (SQLite/PostgreSQL)
      - OM_METADATA_BACKEND=${OM_METADATA_BACKEND:-sqlite}
      - OM_STRICT_TENANT=${OM_STRICT_TENANT:-true}
      - OM_PG_HOST=${OM_PG_HOST:-}
      - OM_PG_PORT=${OM_PG_PORT:-5432}
      - OM_PG_DB=${OM_PG_DB:-}
      - OM_PG_USER=${OM_PG_USER:-}
      - OM_PG_PASSWORD=${OM_PG_PASSWORD:-}
      - OM_PG_SCHEMA=${OM_PG_SCHEMA:-public}
      - OM_PG_TABLE=${OM_PG_TABLE:-openmemory_memories}
      - OM_PG_SSL=${OM_PG_SSL:-disable}

      # Universal Postgres connection string (takes precedence over individual OM_PG_* vars)
      - OM_PG_CONNECTION_STRING=${OM_PG_CONNECTION_STRING:-}

      # Vector Backend (SQLite/Weaviate)
      - OM_VECTOR_BACKEND=${OM_VECTOR_BACKEND:-sqlite}
      - OM_VECTOR_TABLE=${OM_VECTOR_TABLE:-openmemory_vectors}
      - OM_WEAVIATE_URL=${OM_WEAVIATE_URL:-}
      - OM_WEAVIATE_API_KEY=${OM_WEAVIATE_API_KEY:-}
      - OM_WEAVIATE_CLASS=${OM_WEAVIATE_CLASS:-OpenMemory}

      # Auto Reflection
      - OM_AUTO_REFLECT=${OM_AUTO_REFLECT:-false}
      - OM_REFLECT_INTERVAL=${OM_REFLECT_INTERVAL:-10}
      - OM_REFLECT_MIN_MEMORIES=${OM_REFLECT_MIN_MEMORIES:-20}

      # User Summaries
      - OM_USER_SUMMARY_INTERVAL=${OM_USER_SUMMARY_INTERVAL:-30}
      - OM_USE_SUMMARY_ONLY=${OM_USE_SUMMARY_ONLY:-true}
      - OM_SUMMARY_MAX_LENGTH=${OM_SUMMARY_MAX_LENGTH:-200}

      # HSG Configuration
      - OM_SEG_SIZE=${OM_SEG_SIZE:-10000}
      - OM_CACHE_SEGMENTS=${OM_CACHE_SEGMENTS:-3}
      - OM_MAX_ACTIVE=${OM_MAX_ACTIVE:-64}

      # Decay System
      - OM_DECAY_LAMBDA=${OM_DECAY_LAMBDA:-0.02}
      - OM_DECAY_INTERVAL_MINUTES=${OM_DECAY_INTERVAL_MINUTES:-1440}
      - OM_DECAY_RATIO=${OM_DECAY_RATIO:-0.03}
      - OM_DECAY_SLEEP_MS=${OM_DECAY_SLEEP_MS:-200}
      - OM_DECAY_THREADS=${OM_DECAY_THREADS:-3}
      - OM_DECAY_COLD_THRESHOLD=${OM_DECAY_COLD_THRESHOLD:-0.25}
      - OM_DECAY_REINFORCE_ON_QUERY=${OM_DECAY_REINFORCE_ON_QUERY:-true}

      # Compression & Regeneration
      - OM_REGENERATION_ENABLED=${OM_REGENERATION_ENABLED:-true}
      - OM_MAX_VECTOR_DIM=${OM_MAX_VECTOR_DIM:-256}
      - OM_MIN_VECTOR_DIM=${OM_MIN_VECTOR_DIM:-64}
      - OM_SUMMARY_LAYERS=${OM_SUMMARY_LAYERS:-3}

      # Keyword Extraction
      - OM_KEYWORD_BOOST=${OM_KEYWORD_BOOST:-2.5}
      - OM_KEYWORD_MIN_LENGTH=${OM_KEYWORD_MIN_LENGTH:-3}
    volumes:
      - openmemory_data:/data:Z # Rootless: create with `podman volume create openmemory_data --driver local --opt o=uid=$(id -u),gid=$(id -g)`
    # Note: depends_on removed to make Ollama sidecar optional
    # Use `docker compose --profile ollama up` to include Ollama service
    userns_mode: keep-id
    init: true
    stop_grace_period: 30s
    restart: unless-stopped
    healthcheck:
      test:
        [
          "CMD",
          "bun",
          "-e",
          'fetch("http://localhost:8080/health").then(r=>process.exit(r.ok?0:1)).catch(()=>process.exit(1))',
        ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    networks:
      - openmemory_net

  # Ollama - embedding and multimodal inference sidecar
  ollama:
    profiles: ["ollama"]
    image: ollama/ollama:0.3.0
    restart: unless-stopped
    ports:
      - "11434:11434"
    volumes:
      - ollama_models:/root/.ollama:Z
    userns_mode: keep-id
    init: true
    stop_grace_period: 10s
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    # Resource constraints (helps when using Podman or when running locally in tests)
    deploy:
      resources:
        limits:
          cpus: "1.0"
          memory: 1G
    # GPU passthrough examples (uncomment if you run with GPUs)
    # NVIDIA (requires NVIDIA Container Toolkit / CDI)
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: "nvidia"
    #           device_ids: ["all"]
    #           capabilities: ["gpu"]
    # Podman (rootless) example to allow the container to access NVIDIA GPUs:
    #   --device nvidia.com/gpu=all --security-opt label=disable
    # AMD (Vulkan) example (node host must have mesa and Vulkan ICDs):
    #   devices:
    #     - "/dev/dri:/dev/dri"
    #     - "/dev/kfd:/dev/kfd"
    #   volumes:
    #     - /usr/share/vulkan/icd.d:/usr/share/vulkan/icd.d:ro
    # Older `mem_limit`/`cpus` fields are accepted by various compose implementations
    mem_limit: 1G
    cpus: 1
    networks:
      - openmemory_net
    environment:
      - OLLAMA_KEEP_ALIVE=${OM_OLLAMA_KEEP_ALIVE:-5m}
      - OLLAMA_NUM_PARALLEL=${OM_OLLAMA_NUM_PARALLEL:-1}
      - OLLAMA_NUM_GPU=${OM_OLLAMA_NUM_GPU:-0}
      # GPU Tuning (optional): flash attention & kv cache tuning
      - OLLAMA_FLASH_ATTENTION=${OLLAMA_FLASH_ATTENTION:-0}
      - OLLAMA_KV_CACHE_TYPE=${OLLAMA_KV_CACHE_TYPE:-q8_0}
      # - OLLAMA_VULKAN=1  # Enable Vulkan backend for AMD GPUs

  # Optional test runner service (profiles: ["test"]) - use the named profile to enable
  # Use: `docker-compose --profile test up tests` to run the test harness against
  # a local containerized environment. Disabled by default (profile required).
  tests:
    profiles: ["test"]
    image: oven/bun:1.3.2-alpine
    working_dir: /work/backend
    volumes:
      - ./:/work:Z
    command: ["bun", "test", "../tests/backend/"]
    environment:
      - OM_METADATA_BACKEND=${OM_METADATA_BACKEND:-sqlite}
      - OM_STRICT_TENANT=${OM_STRICT_TENANT:-true}
    depends_on:
      - openmemory
    networks:
      - openmemory_net

  # Optional Postgres service for integration testing and local development.
  # Enable with: `docker compose --profile pg up postgres`
  postgres:
    image: postgres:16
    restart: unless-stopped
    environment:
      - POSTGRES_DB=${OM_PG_DB:-openmemory}
      - POSTGRES_USER=${OM_PG_USER:-openmemory}
      - POSTGRES_PASSWORD=${OM_PG_PASSWORD:-openmemory}
    volumes:
      - openmemory_pg_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"
    profiles: ["pg"]
    networks:
      - openmemory_net

volumes:
  openmemory_data:
    driver: local
    # For rootless Podman, consider creating the volume with user-owned uid/gid
    # podman volume create openmemory_data --driver local --opt o=uid=$(id -u),gid=$(id -g)
  ollama_models:
    driver: local
    # For rootless Podman: podman volume create ollama_models --driver local --opt o=uid=$(id -u),gid=$(id -g)
  # Ephemeral volume for test runs (use profile 'test' to enable services that mount this)
  test_data:
    driver: local

networks:
  openmemory_net:
    driver: bridge
